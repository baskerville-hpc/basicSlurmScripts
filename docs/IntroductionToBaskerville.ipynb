{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Introduction to Baskerville: Using Slurm\"\n",
        "author: \"James Allsopp and Baskerville Team\"\n",
        "format:\n",
        "  revealjs:\n",
        "    incremental: false\n",
        "    theme: moon\n",
        "    footer: \"Using Slurm\"\n",
        "    logo: RSG.png\n",
        "    css: logo.css\n",
        "    fig-cap-location: margin\n",
        "    code-line-numbers: true\n",
        "    fontsize: 30px\n",
        "    width: 1500\n",
        "---"
      ],
      "id": "8fd309c0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Slurm - Simple Linux Uniform Resource Manager\n",
        "\n",
        "+ Used to stop and start non-interactive jobs\n",
        "+ Log into a login node via SSH\n",
        "+ Run scripts using **sbatch** command.\n",
        "+ After a time in the queue, your job starts on a compute node\n",
        "\n",
        "All of your data and code is available on all nodes.\n",
        "\n",
        "\n",
        "If you want to follow along with any of the examples, you can find them here -\n",
        "\n",
        " https://github.com/baskerville-hpc/basicSlurmScripts\n",
        "\n",
        "\n",
        "## Simple Slurm script\n",
        "\n",
        "\n",
        "````{.bash  filename=\"basicSlurm.sh\" code-line-numbers=\"|1|2-4|6-7|9-11\"}\n",
        "#!/bin/bash\n",
        "#SBATCH --qos arc\n",
        "#SBATCH --account edmondac-rsg\n",
        "#SBATCH --time 1:0:0\n",
        "\n",
        "module purge\n",
        "module load baskerville    #Find more about apps here; https://apps.baskerville.ac.uk/\n",
        "\n",
        "echo -n \"This script is running on \"\n",
        "hostname\n",
        "sleep 10\n",
        "````\n",
        "\n",
        "+ If you ever need to find your QoS and account, use the command <br/>\n",
        "&nbsp;**my_baskerville**<br/>\n",
        "while logged into a Baskerville SSH connection.\n",
        "+ First seven lines rarely change if you're working on the same project.\n",
        "+ If you want to comment the #SBATCH lines, change to ##SBATCH .....\n",
        "+ No Bash commands before #SBATCH, except **#!/bin/bash**\n",
        "\n",
        "\n",
        "\n",
        "## Simple Slurm script\n",
        "\n",
        "````{.bash filename=\"basicSlurm.sh\"}\n",
        "#!/bin/bash\n",
        "#SBATCH --qos arc\n",
        "#SBATCH --account edmondac-rsg\n",
        "#SBATCH --time 1:0:0\n",
        "\n",
        "module purge\n",
        "module load baskerville\n",
        "\n",
        "echo -n \"This script is running on \"\n",
        "hostname\n",
        "sleep 10\n",
        "````\n",
        "\n",
        "You can run this script on a compute node using;\n",
        "\n",
        "\n",
        "```{bash filename=\"basicSlurm.sh\"}\n",
        "\n",
        "sbatch basicSlurm.sh\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Sbatch outout\n",
        "\n",
        "Two output files produced,\n",
        "\n",
        "### slurm-&lt;job id&gt;.out\n",
        "\n",
        "````{.default}\n",
        "This script is running on bask-pg0308u24a.cluster.baskerville.ac.uk\n",
        "````\n",
        "\n",
        "### slurm-&lt;job id&gt;.stats\n",
        "````{.default}\n",
        "+--------------------------------------------------------------------------+\n",
        "| Job on the Baskerville cluster:\n",
        "| Starting at Tue Jul 25 11:32:17 2023 for allsoppj(836257)\n",
        "| Identity jobid 474749 jobname basicSlurm.sh\n",
        "| Running against project edmondac-rsg and in partition baskerville-a100_40\n",
        "| Requested cpu=2,mem=6G,node=1,billing=2 - 01:00:00 walltime\n",
        "| Assigned to nodes bask-pg0308u24a\n",
        "| Command /bask/homes/a/allsoppj/BaskervilleRemoteDropIn/BasicSlurmFile/basicSlurm.sh\n",
        "| WorkDir /bask/homes/a/allsoppj/BaskervilleRemoteDropIn/BasicSlurmFile\n",
        "+--------------------------------------------------------------------------+\n",
        "+--------------------------------------------------------------------------+\n",
        "| Finished at Tue Jul 25 11:32:37 2023 for allsoppj(836257) on the Baskerville Cluster\n",
        "| Required (00:00.689 cputime, 4232K memory used) - 00:00:20 walltime\n",
        "| JobState COMPLETING - Reason None\n",
        "| Exitcode 0:0\n",
        "+--------------------------------------------------------------------------+\n",
        "````\n",
        "\n",
        "## Checking on a job\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "+ If you want to see how all of your jobs are doing\n",
        "\n",
        "&nbsp;**squeue -u &lt;user name&gt;**\n",
        "\n",
        "+ If you want to see how one particular job is doing\n",
        "\n",
        "&nbsp;**squeue -j &lt;job id&gt;**\n",
        "\n",
        "<br/>\n",
        "For example\n",
        "<br/>\n",
        "````{.default code-line-numbers=\"false\"}\n",
        "[allsoppj@bask-pg0310u18a BasicSlurmFile]$ squeue -j 474735\n",
        "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
        "            474735 baskervil basicSlu allsoppj  R       0:09      1 bask-pg0308u24a\n",
        "````\n",
        "\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "| Code |            |               |\n",
        "|------|------------|---------------|\n",
        "| PD   | Pending    | All good - waiting for resources before starting|\n",
        "| R    | Running    | All good - working away                          |\n",
        "| CG   | Completing | All good - finished but some processes still working|\n",
        "| C    | Completed  | All good - job successfully finished |\n",
        "| F    | Failed     |                                      |\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "## Oh no, I've started a job and need to stop it\n",
        "\n",
        "\n",
        "<br/>\n",
        "Jobs can be stopped at any time using\n",
        "\n",
        "&nbsp;**scancel &lt;job id&gt;** # Note the lack of a \"-j\" for this.\n",
        "<br/>\n",
        "\n",
        "<br/>\n",
        "\n",
        "#### Quick tip\n",
        "\n",
        "\n",
        "Store job id in a bash variable directly using:\n",
        "\n",
        "&nbsp;**job_id = $(sbatch --parsable &lt;slurm file&gt;)**\n",
        "<br/>\n",
        "\n",
        "## Change job name\n",
        "<br/>\n",
        "Job name is used throughout slurm, so change it to something more readable than the script name:\n",
        "\n",
        "````{.bash}\n",
        "#SBATCH --job-name \"Amorereadablename\"\n",
        "````\n",
        "\n",
        "<br/>\n",
        "\n",
        "````{.default code-line-number=\"|7\"}\n",
        "[allsoppj@bask-pg0310u18a BasicSlurmFile]$ cat slurm-474832.out\n",
        "\n",
        "This script is running on bask-pg0308u24a.cluster.baskerville.ac.uk\n",
        "[allsoppj@bask-pg0310u18a BasicSlurmFile]$ cat slurm-474832.stats\n",
        " Job on the Baskerville cluster:\n",
        " Starting at Tue Jul 25 17:37:51 2023 for allsoppj(836257)\n",
        " Identity jobid 474832 jobname Amorereadablename\n",
        " Running against project edmondac-rsg and in partition baskerville-a100_40\n",
        " Requested cpu=2,mem=6G,node=1,billing=2 - 01:00:00 walltime\n",
        " Assigned to nodes bask-pg0308u24a\n",
        " Command /bask/homes/a/allsoppj/BaskervilleRemoteDropIn/BasicSlurmFile/changeName.sh\n",
        " WorkDir /bask/homes/a/allsoppj/BaskervilleRemoteDropIn/BasicSlurmFile\n",
        "\n",
        "\n",
        " Finished at Tue Jul 25 17:38:11 2023 for allsoppj(836257) on the Baskerville Cluster\n",
        " Required (00:00.701 cputime, 4236K memory used) - 00:00:19 walltime\n",
        " JobState COMPLETING - Reason None\n",
        " Exitcode 0:0\n",
        "````\n",
        "\n",
        "## Change the hardware you want you job to run on\n",
        "<br/>\n",
        "Baskerville has two types of GPU,\n",
        "\n",
        "+ A100-40 (default)\n",
        "+ A100-80\n",
        "````{.bash}\n",
        "#SBATCH --constraint=a100_80\n",
        "````\n",
        "\n",
        "## Change the number of nodes or GPUs\n",
        "<br/>\n",
        "````{.bash}\n",
        "#SBATCH --gpus-per-task 3\n",
        "#SBATCH --task-per-node 1\n",
        "#SBATCH --nodes 1\n",
        "````\n",
        "Documented in more detail in the [docs.baskerville.ac.uk](docs.baskerville.ac.uk)\n",
        "\n",
        "## Change location of the output files\n",
        "<br/>\n",
        "\n",
        "````{.bash}\n",
        "#SBATCH --output=./output_file/slurm-%A_%a.out\n",
        "````\n",
        "\n",
        "Default option for this is <br/>&nbsp;**slurm-%j.out**\n",
        "<br/>\n",
        "\n",
        "Full list of options at https://doc.hpc.iter.es/slurm/how_to_slurm_filenamepatterns\n",
        "\n",
        "\n",
        "Don't try **#SBATCH --output $(pwd)/outputfiles/%A_%a.out**, it won't work.\n",
        "\n",
        "\n",
        "## Slurm Arrays\n",
        "\n",
        "<br/>\n",
        "\n",
        "Run many jobs from one Slurm file\n",
        "\n",
        "````{.bash}\n",
        "#SBATCH --array=1-10%2\n",
        "````\n",
        "\n",
        "Adding this will run the script 10 times with 2 jobs running simultaneously\n",
        "\n",
        "Need to use with environment variables to make it useful.\n",
        "\n",
        "+ SLURM_JOB_ID          - Full job number for each array job\n",
        "+ SLURM_ARRAY_JOB_ID    - Job id of the array itself\n",
        "+ SLURM_ARRAY_TASK_ID   - Index of the job in the array, e.g. a value between 1 and 10 from above.\n",
        "\n",
        "To track these jobs use the **sacct -j &lt;Job id&gt;** command, or the squeue command.\n",
        "\n",
        "\n",
        "## Two questions about Slurm Arrays\n",
        "\n",
        "\n",
        "+ How do I give different values to each job?\n",
        "  - Use the SLURM_ARRAY_TASK_ID as a lookup to the information you want to use.\n",
        "\n",
        "+ How do I make a Slurm array bigger than MaxArraySize?\n",
        "  - Set to 4000 on BlueBEAR, defaults to 1001.\n",
        "  - Use a script to repeatedly launch a slurm job with an increasing offset each time 1,2,3.....\n",
        "     * Slurm scripts can take command line arguments\n",
        "     * Add this to SLURM_ARRAY_TASK_ID before doing the lookup.\n",
        "\n",
        "\n",
        "Used this approach to run nearly 700,000 jobs in blocks of 4000, 500 at a time."
      ],
      "id": "cefd1f24"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}